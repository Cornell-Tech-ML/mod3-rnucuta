MAP

================================================================================
 Parallel Accelerator Optimizing:  Function tensor_map.<locals>._map,
/mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py
(163)
================================================================================


Parallel loop listing for  Function tensor_map.<locals>._map, /mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py (163)
-------------------------------------------------------------------------------|loop #ID
    def _map(                                                                  |
        out: Storage,                                                          |
        out_shape: Shape,                                                      |
        out_strides: Strides,                                                  |
        in_storage: Storage,                                                   |
        in_shape: Shape,                                                       |
        in_strides: Strides,                                                   |
    ) -> None:                                                                 |
        # TODO: Implement for Task 3.1.                                        |
        # fn = njit()(fn)                                                      |
        # Main loop in parallel                                                |
        # All indices use numpy buffers                                        |
        if len(out_shape) > MAX_DIMS or len(in_shape) > MAX_DIMS:              |
            raise ValueError(f"Tensor dimensions cannot exceed {MAX_DIMS}")    |
                                                                               |
        # When out and in are stride-aligned, avoid indexing                   |
        if (                                                                   |
            len(out_shape) == len(in_shape)                                    |
            and len(out_strides) == len(in_strides)                            |
            and (out_shape == in_shape).all()----------------------------------| #0
            and (out_strides == in_strides).all()------------------------------| #1
        ):                                                                     |
            for i in prange(len(out)):-----------------------------------------| #2
                out[i] = fn(in_storage[i])                                     |
                                                                               |
        else:                                                                  |
            for i in prange(len(out)):-----------------------------------------| #3
                out_index: Index = np.empty(MAX_DIMS, dtype=np.int32)          |
                in_index: Index = np.empty(MAX_DIMS, dtype=np.int32)           |
                # Convert flat index to multidimensional index                 |
                # to_index function is useful just for getting                 |
                # continous set of indices                                     |
                to_index(i, out_shape, out_index)                              |
                                                                               |
                broadcast_index(out_index, out_shape, in_shape, in_index)      |
                                                                               |
                # ordinals                                                     |
                in_position = index_to_position(in_index, in_strides)          |
                out_position = index_to_position(out_index, out_strides)       |
                                                                               |
                out[out_position] = fn(in_storage[in_position])                |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 4 parallel for-
loop(s) (originating from loops labelled: #0, #1, #2, #3).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
The memory allocation derived from the instruction at
/mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py
(190) is hoisted out of the parallel loop labelled #3 (it will be performed
before the loop is executed and reused inside the loop):
   Allocation:: out_index: Index = np.empty(MAX_DIMS, dtype=np.int32)
    - numpy.empty() is used for the allocation.
The memory allocation derived from the instruction at
/mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py
(191) is hoisted out of the parallel loop labelled #3 (it will be performed
before the loop is executed and reused inside the loop):
   Allocation:: in_index: Index = np.empty(MAX_DIMS, dtype=np.int32)
    - numpy.empty() is used for the allocation.
None
ZIP

================================================================================
 Parallel Accelerator Optimizing:  Function tensor_zip.<locals>._zip,
/mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py
(231)
================================================================================


Parallel loop listing for  Function tensor_zip.<locals>._zip, /mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py (231)
-------------------------------------------------------------------------------|loop #ID
    def _zip(                                                                  |
        out: Storage,                                                          |
        out_shape: Shape,                                                      |
        out_strides: Strides,                                                  |
        a_storage: Storage,                                                    |
        a_shape: Shape,                                                        |
        a_strides: Strides,                                                    |
        b_storage: Storage,                                                    |
        b_shape: Shape,                                                        |
        b_strides: Strides,                                                    |
    ) -> None:                                                                 |
        # TODO: Implement for Task 3.1.                                        |
        if (                                                                   |
            len(out_shape) > MAX_DIMS                                          |
            or len(a_shape) > MAX_DIMS                                         |
            or len(b_shape) > MAX_DIMS                                         |
        ):                                                                     |
            raise ValueError(f"Tensor dimensions cannot exceed {MAX_DIMS}")    |
                                                                               |
        n = len(out)                                                           |
        if (                                                                   |
            len(out_strides) == len(a_strides)                                 |
            and len(out_shape) == len(a_shape)                                 |
            and len(a_strides) == len(b_strides)                               |
            and len(b_shape) == len(a_shape)                                   |
            and (out_strides == a_strides).all()-------------------------------| #4
            and (a_strides == b_strides).all()---------------------------------| #5
            and (out_shape == a_shape).all()-----------------------------------| #6
            and (a_shape == b_shape).all()-------------------------------------| #7
        ):                                                                     |
            for i in prange(n):------------------------------------------------| #8
                out[i] = fn(a_storage[i], b_storage[i])                        |
        else:                                                                  |
            # global broacasting case                                          |
            for i in prange(n):------------------------------------------------| #9
                out_index = np.empty(MAX_DIMS, dtype=np.int32)                 |
                in_index_a = np.empty(MAX_DIMS, dtype=np.int32)                |
                in_index_b = np.empty(MAX_DIMS, dtype=np.int32)                |
                to_index(i, out_shape, out_index)                              |
                                                                               |
                broadcast_index(out_index, out_shape, a_shape, in_index_a)     |
                broadcast_index(out_index, out_shape, b_shape, in_index_b)     |
                                                                               |
                # Calculate ordinals                                           |
                in_position_a = index_to_position(in_index_a, a_strides)       |
                in_position_b = index_to_position(in_index_b, b_strides)       |
                out_position = index_to_position(out_index, out_strides)       |
                                                                               |
                out[out_position] = fn(                                        |
                    a_storage[in_position_a], b_storage[in_position_b]         |
                )                                                              |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 6 parallel for-
loop(s) (originating from loops labelled: #4, #5, #6, #7, #8, #9).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
The memory allocation derived from the instruction at
/mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py
(266) is hoisted out of the parallel loop labelled #9 (it will be performed
before the loop is executed and reused inside the loop):
   Allocation:: out_index = np.empty(MAX_DIMS, dtype=np.int32)
    - numpy.empty() is used for the allocation.
The memory allocation derived from the instruction at
/mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py
(267) is hoisted out of the parallel loop labelled #9 (it will be performed
before the loop is executed and reused inside the loop):
   Allocation:: in_index_a = np.empty(MAX_DIMS, dtype=np.int32)
    - numpy.empty() is used for the allocation.
The memory allocation derived from the instruction at
/mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py
(268) is hoisted out of the parallel loop labelled #9 (it will be performed
before the loop is executed and reused inside the loop):
   Allocation:: in_index_b = np.empty(MAX_DIMS, dtype=np.int32)
    - numpy.empty() is used for the allocation.
None
REDUCE

================================================================================
 Parallel Accelerator Optimizing:  Function tensor_reduce.<locals>._reduce,
/mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py
(307)
================================================================================


Parallel loop listing for  Function tensor_reduce.<locals>._reduce, /mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py (307)
-------------------------------------------------------------------------------|loop #ID
    def _reduce(                                                               |
        out: Storage,                                                          |
        out_shape: Shape,                                                      |
        out_strides: Strides,                                                  |
        a_storage: Storage,                                                    |
        a_shape: Shape,                                                        |
        a_strides: Strides,                                                    |
        reduce_dim: int,                                                       |
    ) -> None:                                                                 |
        # TODO: Implement for Task 3.1.                                        |
        if len(out_shape) > MAX_DIMS or len(a_shape) > MAX_DIMS:               |
            raise ValueError(f"Tensor dimensions cannot exceed {MAX_DIMS}")    |
                                                                               |
        for i in prange(len(out)):---------------------------------------------| #10
            out_index: Index = np.empty(MAX_DIMS, dtype=np.int32)              |
            reduce_size: int = a_shape[reduce_dim]                             |
            to_index(i, out_shape, out_index)                                  |
            output_position = index_to_position(out_index, out_strides)        |
                                                                               |
            reduce_stride = a_strides[reduce_dim]                              |
            baseIdx = index_to_position(out_index, a_strides)                  |
            acc = out[output_position]                                         |
            for j in range(reduce_size):                                       |
                a_position = baseIdx + j * reduce_stride                       |
                acc = fn(acc, a_storage[a_position])                           |
                                                                               |
            out[output_position] = acc                                         |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 1 parallel for-
loop(s) (originating from loops labelled: #10).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
The memory allocation derived from the instruction at
/mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py
(321) is hoisted out of the parallel loop labelled #10 (it will be performed
before the loop is executed and reused inside the loop):
   Allocation:: out_index: Index = np.empty(MAX_DIMS, dtype=np.int32)
    - numpy.empty() is used for the allocation.
None
MATRIX MULTIPLY

================================================================================
 Parallel Accelerator Optimizing:  Function _tensor_matrix_multiply,
/mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py
(338)
================================================================================


Parallel loop listing for  Function _tensor_matrix_multiply, /mnt/c/Users/raymo/Documents/CTS1/CS5781/mod3-rnucuta/minitorch/fast_ops.py (338)
--------------------------------------------------------------------------------------------|loop #ID
def _tensor_matrix_multiply(                                                                |
    out: Storage,                                                                           |
    out_shape: Shape,                                                                       |
    out_strides: Strides,                                                                   |
    a_storage: Storage,                                                                     |
    a_shape: Shape,                                                                         |
    a_strides: Strides,                                                                     |
    b_storage: Storage,                                                                     |
    b_shape: Shape,                                                                         |
    b_strides: Strides,                                                                     |
) -> None:                                                                                  |
    """NUMBA tensor matrix multiply function.                                               |
                                                                                            |
    Should work for any tensor shapes that broadcast as long as                             |
                                                                                            |
    ```                                                                                     |
    assert a_shape[-1] == b_shape[-2]                                                       |
    ```                                                                                     |
                                                                                            |
    Optimizations:                                                                          |
                                                                                            |
    * Outer loop in parallel                                                                |
    * No index buffers or function calls                                                    |
    * Inner loop should have no global writes, 1 multiply.                                  |
                                                                                            |
                                                                                            |
    Args:                                                                                   |
    ----                                                                                    |
        out (Storage): storage for `out` tensor                                             |
        out_shape (Shape): shape for `out` tensor                                           |
        out_strides (Strides): strides for `out` tensor                                     |
        a_storage (Storage): storage for `a` tensor                                         |
        a_shape (Shape): shape for `a` tensor                                               |
        a_strides (Strides): strides for `a` tensor                                         |
        b_storage (Storage): storage for `b` tensor                                         |
        b_shape (Shape): shape for `b` tensor                                               |
        b_strides (Strides): strides for `b` tensor                                         |
                                                                                            |
    Returns:                                                                                |
    -------                                                                                 |
        None : Fills in `out`                                                               |
                                                                                            |
    """                                                                                     |
    # TODO: Implement for Task 3.2.                                                         |
    assert a_shape[-1] == b_shape[-2], "a_shape[-1] != b_shape[-2]"                         |
    a_batch_stride = a_strides[0] if a_shape[0] > 1 else 0                                  |
    b_batch_stride = b_strides[0] if b_shape[0] > 1 else 0                                  |
    out_batch_stride = out_strides[0] if out_shape[0] > 1 else 0                            |
    for batch in prange(out_shape[0]):------------------------------------------------------| #11
        batch_idx_a = batch * a_batch_stride                                                |
        batch_idx_b = batch * b_batch_stride                                                |
        out_batch_idx = batch * out_batch_stride                                            |
        for r in range(out_shape[-2]):                                                      |
            for c in range(out_shape[-1]):                                                  |
                acc = 0.0                                                                   |
                for i in range(a_shape[-1]):  # want to do the inner loop before col        |
                    # like BLAS implementation to be more cache friendly,                   |
                    # but this causes global writes                                         |
                    acc += (                                                                |
                        a_storage[batch_idx_a + r * a_strides[-2] + i * a_strides[-1]]      |
                        * b_storage[batch_idx_b + i * b_strides[-2] + c * b_strides[-1]]    |
                    )                                                                       |
                out[out_batch_idx + r * out_strides[-2] + c * out_strides[-1]] = acc        |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 1 parallel for-
loop(s) (originating from loops labelled: #11).
--------------------------------------------------------------------------------
----------------------------- Before Optimisation ------------------------------
--------------------------------------------------------------------------------
------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found
None
